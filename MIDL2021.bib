@Proceedings{MIDL2021,
  name =	 {Medical Imaging with Deep Learning},
  booktitle =	 {Proceedings of the Fourth Conference on Medical
                  Imaging with Deep Learning},
  shortname =	 {MIDL},
  conference_number =4,
  year =	 2021,
  start =	 {2021-07-07},
  end =		 {2021-07-09},
  published =	 {2021-08-25},
  address =	 {L{\"u}beck, Germany},
  conference_url ={https://2021.midl.io},
  editor =	 {Heinrich, Mattias and Dou, Qi and de Bruijne,
                  Marleen and Lellmann, Jan and Schl{\"a}fer,
                  Alexander and Ernst, Floris},
  publisher =	 {PMLR},
  series =	 {Proceedings of Machine Learning Research},
  volume =	 143
}



@InProceedings{preface,
  author =	 {Heinrich, Mattias and Dou, Qi and de Bruijne,
                  Marleen and Lellmann, Jan and Schl{\"a}fer,
                  Alexander and Ernst, Floris},
  title = 	 {Preface},
  pages =	 {1--4},
  abstract = 	 {Introduction to the volume from the editors.}
}

@inproceedings{abbet21,
  abstract =	 {Supervised learning is conditioned by the
                  availability of labeled data, which are especially
                  expensive to acquire in the field of medical image
                  analysis. Making use of open-source data for
                  pre-training or using domain adaptation can be a way
                  to overcome this issue. However, pre-trained
                  networks often fail to generalize to new test
                  domains that are not distributed identically due to
                  variations in tissue stainings, types, and
                  textures. Additionally, current domain adaptation
                  methods mainly rely on fully-labeled source
                  datasets. In this work, we propose Self-Rule to
                  Adapt (SRA) which takes advantage of self-supervised
                  learning to perform domain adaptation and removes
                  the burden of fully-labeled source datasets. SRA can
                  effectively transfer the discriminative knowledge
                  obtained from a few labeled source domain to a new
                  target domain without requiring additional tissue
                  annotations. Our method harnesses both domains'
                  structures by capturing visual similarity with
                  intra-domain and cross-domain self-supervision. We
                  show that our proposed method outperforms baselines
                  across diverse domain adaptation settings and
                  further validate our approach to our in-house
                  clinical cohort.},
  author =	 {Abbet, Christian and Studer, Linda and Fischer,
                  Andreas and Dawson, Heather and Zlobec, Inti and
                  Bozorgtabar, Behzad and Thiran, Jean-Philippe},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {5--21},
  title =	 {Self-Rule to Adapt: Learning Generalized Features
                  from Sparsely-Labeled Data Using Unsupervised Domain
                  Adaptation for Colorectal Cancer Tissue Phenotyping},
  openreview =	 {VO7asaS5GUk},
}

@inproceedings{amador21,
  abstract =	 {Acute ischemic stroke is caused by a blockage in the
                  cerebral arteries, resulting in long-term disability
                  and sometimes death. To determine the optimal
                  treatment strategy, a patient-specific assessment is
                  often based on advanced neuroimaging data, such as
                  spatio-temporal (4D) CT Perfusion (CTP) imaging. To
                  date, perfusion maps are typically calculated from
                  4D CTP data and then thresholded to localize and
                  quantify the stroke lesion core and
                  tissue-at-risk. A few studies have recently
                  developed deep learning methods to predict stroke
                  lesion outcomes from perfusion maps. The basic idea
                  of these is to train a model, using perfusion maps
                  acquired at baseline and their corresponding
                  follow-up images acquired several days after
                  treatment, to automatically estimate the final
                  lesion location and volume in new
                  patients. Nevertheless, model training based on the
                  original 4D CTP scans might be desirable, as they
                  could contain more valuable information not directly
                  represented in perfusion maps. Therefore, we aimed
                  to develop and evaluate a temporal convolutional
                  neural network (TCN) to predict stroke lesion
                  outcomes directly from 4D CTP datasets acquired at
                  admission, without computing any perfusion
                  maps. Using a total of 176 CTP scans, we
                  investigated the impact of the time window size by
                  training the proposed TCN on various numbers of CTP
                  frames: 8, 16, and 32 time points. For comparison
                  purposes, we also trained a convolutional neural
                  network based on perfusion maps. The results show
                  that the model trained on 32 time points yielded
                  significantly higher Dice values (0.33$\pm$0.21)
                  than the models trained on 8 time points
                  (0.25$\pm$0.20; P$<$0.05), 16 time points
                  (0.28$\pm$0.21; P$<$0.001), and perfusion maps
                  (0.23$\pm$0.18; P$<$0.05). These experiments
                  demonstrate that the proposed model effectively
                  extracts spatio-temporal data from CTP scans to
                  predict stroke lesion outcomes, which leads to
                  better results than using perfusion maps.},
  author =	 {Amador, Kimberly and Wilms, Matthias and Winder,
                  Anthony and Fiehler, Jens and Forkert, Nils},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {22--33},
  title =	 {Stroke Lesion Outcome Prediction Based on 4D {CT}
                  Perfusion Data Using Temporal Convolutional
                  Networks},
  openreview =	 {0YDEgvfwEW},
}

@inproceedings{bargsten21,
  abstract =	 {Using intracoronary imaging modalities like
                  intravascular ultrasound (IVUS) has a positive
                  impact on the results of percutaneous coronary
                  interventions. Efficient extraction of important
                  vessel metrics like lumen diameter, vessel wall
                  thickness or plaque burden via automatic
                  segmentation of IVUS images can improve the clinical
                  workflow. State-of-the-art segmentation results are
                  usually achieved by data-driven methods like
                  convolutional neural networks (CNNs). However,
                  clinical data sets are often rather small leading to
                  extraction of image features which are not very
                  meaningful and thus decreasing performance. This is
                  also the case for some applications which inherently
                  allow for only small amounts of available data,
                  e.g., detection of diseases with extremely small
                  prevalence or online-adaptation of an existing
                  algorithm to individual patients. In this work we
                  investigate how integrating scattering
                  transformations - as special forms of wavelet
                  transformations - into CNNs could improve the
                  extraction of meaningful features. To this end, we
                  developed a novel network module which uses features
                  of a scattering transform for an attention
                  mechanism. We observed that this approach improves
                  the results of calcium segmentation up to 8.2\%
                  (relatively) in terms of the Dice coefficient and
                  24.8\% in terms of the modified Hausdorff
                  distance. In the case of lumen and vessel wall
                  segmentation, the improvements are up to 2.3\%
                  (relatively) in terms of the Dice coefficient and
                  30.8\% in terms of the modified Hausdorff
                  distance.Incorporating scattering transformations as
                  a component of an attention block into CNNs improves
                  the segmentation results on small IVUS segmentation
                  data sets. In general, scattering transformations
                  can help in situations where efficient feature
                  extractors can not be learned via the training
                  data. This makes our attention module an interesting
                  candidate for applications like few-shot learning
                  for patient adaptation or detection of rare
                  diseases.},
  author =	 {Bargsten, Lennart and Riedl, Katharina A. and
                  Wissel, Tobias and Brunner, Fabian J. and Schaefers,
                  Klaus and Grass, Michael and Blankenberg, Stefan and
                  Seiffert, Moritz and Schlaefer, Alexander},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {34--47},
  title =	 {Attention via Scattering Transforms for Segmentation
                  of Small Intravascular Ultrasound Data Sets},
  openreview =	 {GDs7V3mS1h9},
}

@inproceedings{brudfors21,
  abstract =	 {While convolutional neural networks (CNNs) trained
                  by back-propagation have seen unprecedented success
                  at semantic segmentation tasks, they are known to
                  struggle on out-of-distribution data. Markov random
                  fields (MRFs) on the other hand, encode simpler
                  distributions over labels that, although less
                  flexible than UNets, are less prone to
                  over-fitting. In this paper, we propose to fuse both
                  strategies by computing the product of distributions
                  of a UNet and an MRF. As this product is
                  intractable, we solve for an approximate
                  distribution using an iterative mean-field
                  approach. The resulting MRF-UNet is trained jointly
                  by back-propagation. Compared to other works using
                  conditional random fields (CRFs), the MRF has no
                  dependency on the imaging data, which should allow
                  for less over-fitting. We show on 3D neuroimaging
                  data that this novel network improves generalisation
                  to out-of-distribution samples. Furthermore, it
                  allows the overall number of parameters to be
                  reduced while preserving high accuracy. These
                  results suggest that a classic MRF smoothness prior
                  can allow for less over-fitting when principally
                  integrated into a CNN model. Our implementation is
                  available at https://github.com/balbasty/nitorch.},
  author =	 {Brudfors, Mikael and Balbastre, Ya{\"e}l and
                  Ashburner, John and Rees, Geraint and Nachev,
                  Parashkev and Ourselin, Sebastien and Cardoso,
                  M. Jorge},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {48--59},
  title =	 {An {MRF}-{UN}et Product of Experts for Image
                  Segmentation},
  openreview =	 {PoIV8EXQDjn},
}

@inproceedings{chuli21,
  abstract =	 {We introduce the concept of multi-task learning to
                  weakly-supervised lesion segmentation, one of the
                  most critical and challenging tasks in medical
                  imaging. Due to the lesions' heterogeneous nature,
                  it is difficult for machine learning models to
                  capture the corresponding variability. We propose to
                  jointly train a lesion segmentation model and a
                  lesion classifier in a multi-task learning fashion,
                  where the supervision of the latter is obtained by
                  clustering the RECIST measurements of the
                  lesions. We evaluate our approach specifically on
                  liver lesion segmentation and more generally on
                  lesion segmentation in computed tomography (CT), as
                  well as segmentation of skin lesions from
                  dermatoscopic images. We show that the proposed
                  joint training improves the quality of the lesion
                  segmentation by 4\% percent according to the Dice
                  coefficient and 6\% according to averaged Hausdorff
                  distance (AVD), while reducing the training time
                  required by up to 75\%.},
  author =	 {Chu, Tianshu and Li, Xinmeng and Vo, Huy V. and
                  Summers, Ronald M. and Sizikova, Elena},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {60--73},
  title =	 {Improving Weakly Supervised Lesion Segmentation
                  using Multi-Task Learning},
  openreview =	 {-9bAYexxLtN},
}

@inproceedings{cohen21,
  abstract =	 {Motivation: Traditional image attribution methods
                  struggle to satisfactorily explain predictions of
                  neural networks. Prediction explanation is
                  important, especially in medical imaging, for
                  avoiding the unintended consequences of deploying AI
                  systems when false positive predictions can impact
                  patient care. Thus, there is a pressing need to
                  develop improved models for model explainability and
                  introspection.  Specific problem: A new approach is
                  to transform input images to increase or decrease
                  features which cause the prediction. However,
                  current approaches are difficult to implement as
                  they are monolithic or rely on GANs. These hurdles
                  prevent wide adoption. Our approach: Given an
                  arbitrary classifier, we propose a simple
                  autoencoder and gradient update (Latent Shift) that
                  can transform the latent representation of a
                  specific input image to exaggerate or curtail the
                  features used for prediction. We use this method to
                  study chest X-ray classifiers and evaluate their
                  performance. We conduct a reader study with two
                  radiologists assessing 240 chest X-ray predictions
                  to identify which ones are false positives (half
                  are) using traditional attribution maps or our
                  proposed method. Results: We found low overlap with
                  ground truth pathology masks for models with
                  reasonably high accuracy. However, the results from
                  our reader study indicate that these models are
                  generally looking at the correct features. We also
                  found that the Latent Shift explanation allows a
                  user to have more confidence in true positive
                  predictions compared to traditional approaches
                  (0.15$\pm$0.95 in a 5 point scale with p=0.01) with
                  only a small increase in false positive predictions
                  (0.04$\pm$1.06 with p=0.57). Accompanying webpage:
                  https://mlmed.org/gifsplanation/ Source code:
                  https://github.com/mlmed/gifsplanation},
  author =	 {Cohen, Joseph Paul and Brooks, Rupert and En, Sovann
                  and Zucker, Evan and Pareek, Anuj and Lungren,
                  Matthew P. and Chaudhari, Akshay},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {74--104},
  title =	 {Gifsplanation via Latent Shift: A Simple Autoencoder
                  Approach to Counterfactual Generation for Chest
                  X-rays},
  openreview =	 {rnunjvgxAMt},
}

@inproceedings{czolbe21,
  abstract =	 {We propose a semantic similarity metric for image
                  registration. Existing metrics like euclidean
                  distance or normalized cross-correlation focus on
                  aligning intensity values, giving difficulties with
                  low intensity contrast or noise. Our approach learns
                  dataset-specific features that drive the
                  optimization of a learning-based registration
                  model. We train both an unsupervised approach using
                  an auto-encoder, and a semi-supervised approach
                  using supplemental segmentation data to extract
                  semantic features for image registration. Comparing
                  to existing methods across multiple image modalities
                  and applications, we achieve consistently high
                  registration accuracy. A learned invariance to noise
                  gives smoother transformations on low-quality
                  images.},
  author =	 {Czolbe, Steffen and Krause, Oswin and Feragen, Aasa},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {105--118},
  title =	 {Semantic similarity metrics for learned image
                  registration},
  openreview =	 {9M5cH--UdcC},
}

@inproceedings{dsouza21,
  abstract =	 {We propose a multimodal graph convolutional network
                  (M-GCN) that integrates resting-state fMRI
                  connectivity and diffusion tensor imaging
                  tractography to predict phenotypic measures. Our
                  specialized M-GCN filters act topologically on the
                  functional connectivity matrices, as guided by the
                  subject-wise structural connectomes. The inclusion
                  of structural information also acts as a regularizer
                  and helps extract rich data embeddings that are
                  predictive of clinical outcomes. We validate our
                  framework on 275 healthy individuals from the Human
                  Connectome Project and 57 individuals diagnosed with
                  Autism Spectrum Disorder from an in-house data to
                  predict cognitive measures and behavioral deficits
                  respectively. We demonstrate that the M-GCN
                  outperforms several state-of-the-art baselines in a
                  five-fold cross validated setting and extracts
                  predictive biomarkers from both healthy and autistic
                  populations. Our framework thus provides the
                  representational flexibility to exploit the
                  complementary nature of structure and function and
                  map this information to phenotypic measures in the
                  presence of limited training data.},
  author =	 {Dsouza, Niharika Shimona and Nebel, Mary Beth and
                  Crocetti, Deana and Robinson, Joshua and Mostofsky,
                  Stewart and Venkataraman, Archana},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {119--130},
  title =	 {M-{GCN}: A Multimodal Graph Convolutional Network to
                  Integrate Functional and Structural Connectomics
                  Data to Predict Multidimensional Phenotypic
                  Characterizations},
  openreview =	 {ud-iBiED9zb},
}

@inproceedings{dunnhofer21,
  abstract =	 {This paper presents MRPyrNet, a new convolutional
                  neural network (CNN) architecture that improves the
                  capabilities of CNN-based pipelines for knee injury
                  detection via magnetic resonance imaging
                  (MRI). Existing works showed that anomalies are
                  localized in small-sized knee regions that appear in
                  particular areas of MRI scans. Based on such facts,
                  MRPyrNet exploits a Feature Pyramid Network to
                  enhance small appearing features and Pyramidal
                  Detail Pooling to capture such relevant information
                  in a robust way. Experimental results on two
                  publicly available datasets demonstrate that
                  MRPyrNet improves the ACL tear and meniscal tear
                  diagnosis capabilities of two state-of-the-art
                  methodologies. Code is available at
                  https://git.io/JtMPH.},
  author =	 {Dunnhofer, Matteo and Martinel, Niki and Micheloni,
                  Christian},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {131--147},
  title =	 {Improving {MRI}-based Knee Disorder Diagnosis with
                  Pyramidal Feature Details},
  openreview =	 {7psPmlNffvg},
}

@inproceedings{dzyubachyk21,
  abstract =	 {Intensity of acquired electron microscopy data is
                  subjected to large variability due to the interplay
                  of many different factors, such as microscope and
                  camera settings used for data acquisition, sample
                  thickness, specimen staining protocol and more. In
                  this work, we developed an efficient method for
                  performing intensity inhomogeneity correction on a
                  single set of combined transmission electron
                  microscopy (TEM) images and demonstrated its
                  positive impact on training a neural network on
                  these data. In addition, we investigated what impact
                  different intensity standardization methods have on
                  the training performance, both for data originating
                  from a single source as well as from several
                  different sources. As a concrete example, we
                  considered the problem of segmenting mitochondria
                  from EM data and demonstrated that we were able to
                  obtain promising results when training our network
                  on a large array of highly-variable in-house TEM
                  data.},
  author =	 {Dzyubachyk, Oleh and Koning, Roman I and Mulder, Aat
                  A and Avramut, M. Christina and Faas, Frank GA and
                  Koster, Abraham J},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {148--157},
  title =	 {Intensity Correction and Standardization for
                  Electron Microscopy Data},
  openreview =	 {MAUkVcDzDPA},
}

@inproceedings{eljurdi21,
  abstract =	 {Deep convolutional networks recently made many
                  breakthroughs in medical image segmentation. Still,
                  some anatomical artefacts may be observed in the
                  segmentation results, with holes or inaccuracies
                  near the object boundaries. To address these issues,
                  loss functions that incorporate constraints, such as
                  spatial information or prior knowledge, have been
                  introduced. An example of such prior losses are the
                  contour-based losses, which exploit distance maps to
                  conduct point-by-point optimization between
                  ground-truth and predicted contours. However, such
                  losses may be computationally expensive or
                  susceptible to trivial local solutions and vanishing
                  gradient problems. Moreover, they depend on distance
                  maps which tend to underestimate the
                  contour-to-contour distances. We propose a novel
                  loss constraint that optimizes the perimeter length
                  of the segmented object relative to the ground-truth
                  segmentation. The novelty lies in computing the
                  perimeter with a soft approximation of the contour
                  of the probability map via specialized non-trainable
                  layers in the network. Moreover, we optimize the
                  mean squared error between the predicted perimeter
                  length and ground-truth perimeter length. This soft
                  optimization of contour boundaries allows the
                  network to take into consideration border
                  irregularities within organs while still being
                  efficient. Our experiments on three public datasets
                  (spleen, hippocampus and cardiac structures) show
                  that the proposed method outperforms
                  state-of-the-art boundary losses for both single and
                  multi-organ segmentation.},
  author =	 {{EL Jurdi}, Rosana and Petitjean, Caroline and
                  Honeine, Paul and Cheplygina, Veronika and Abdallah,
                  Fahed},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {158--167},
  title =	 {A Surprisingly Effective Perimeter-based Loss for
                  Medical Image Segmentation},
  openreview =	 {NDEmtyb4cXu},
}

@inproceedings{faryna21,
  abstract =	 {Convolutional neural networks (CNN) are sensitive to
                  domain shifts, which can result in poor
                  generalization. In medical imaging, data acquisition
                  conditions differ among institutions, which leads to
                  variations in image properties and thus domain
                  shift. Stain variation in histopathological slides
                  is a prominent example. Data augmentation is one way
                  to make CNNs robust to varying forms of domain
                  shift, but requires extensive hyperparameter
                  tuning. Due to the large search space, this is
                  cumbersome and often leads to sub-optimal
                  generalization performance. In this work, we focus
                  on automated and computationally efficient data
                  augmentation policy selection for histopathological
                  slides. Building upon the RandAugment framework, we
                  introduce several domain-specific modifications
                  relevant to histopathological images, increasing
                  generalizability. We test these modifications on
                  H\&E-stained histopathology slides from Camelyon17
                  dataset. Our proposed framework outperforms the
                  state-of-the-art manually engineered data
                  augmentation strategy, achieving an area under the
                  ROC curve of 0.964 compared to 0.958, respectively.},
  author =	 {Faryna, Khrystyna and van der Laak, Jeroen and
                  Litjens, Geert},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {168--178},
  title =	 {Tailoring automated data augmentation to
                  H\&E-stained histopathology},
  openreview =	 {JrBfXaoxbA2},
}

@inproceedings{feng21,
  abstract =	 {The tumor microenvironment is an area of intense
                  interest in cancer research and may be a clinically
                  actionable aspect of cancer care. One way to study
                  the tumor microenvironment is to characterize the
                  spatial interactions between various types of nuclei
                  in cancer tissue from H&E whole slide images, which
                  require nucleus segmentation and
                  classification. Current methods of nucleus
                  classification rely on extensive labeling from
                  pathologists and are limited by the number of
                  categories a nucleus can be classified into. In this
                  work, leveraging existing nucleus segmentation and
                  contrastive representation learning methods, we
                  developed a method that learns vector embeddings of
                  nuclei based on their morphology in histopathology
                  images. We show that the embeddings learned by this
                  model capture distinctive morphological features of
                  nuclei and can be used to group them into meaningful
                  subtypes. These embeddings can provide a much richer
                  characterization of the statistics of the spatial
                  distribution of nuclei in cancer and open new
                  possibilities in the quantitative study of the tumor
                  microenvironment.},
  author =	 {Feng, Chao and Vanderbilt, Chad and Fuchs, Thomas},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {179--189},
  title =	 {Nuc2Vec: Learning Representations of Nuclei in
                  Histopathology Images with Contrastive Loss},
  openreview =	 {uLtYvtWw8PH},
}

@inproceedings{gadgil21,
  abstract =	 {Medical image segmentation models are typically
                  supervised by expert annotations at the pixel-level,
                  which can be expensive to acquire. In this work, we
                  propose a method that combines the high quality of
                  pixel-level expert annotations with the scale of
                  coarse DNN-generated saliency maps for training
                  multi-label semantic segmentation models. We
                  demonstrate the application of our semi-supervised
                  method, which we call CheXseg, on multi-label chest
                  X-ray interpretation. We find that CheXseg improves
                  upon the performance (mIoU) of fully-supervised
                  methods that use only pixel-level expert annotations
                  by 9.7\% and weakly-supervised methods that use only
                  DNN-generated saliency maps by 73.1\%. Our best
                  method is able to match radiologist agreement on
                  three out of ten pathologies and reduces the overall
                  performance gap by 57.2\% as compared to
                  weakly-supervised methods.},
  author =	 {Gadgil, Soham Uday and Endo, Mark and Wen, Emily and
                  Ng, Andrew Y. and Rajpurkar, Pranav},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {190--204},
  title =	 {CheXseg: Combining Expert Annotations with
                  {DNN}-generated Saliency Maps for X-ray
                  Segmentation},
  openreview =	 {eA7PGMYmHFA},
}

@inproceedings{gonzalez21,
  abstract =	 {The segmentation of cardiac structures in Cine
                  Magnetic Resonance imaging (CMR) plays an important
                  role in monitoring ventricular function, and many
                  deep learning solutions have been introduced that
                  successfully automate this task. Yet due to
                  variabilities in the CMR acquisition process, images
                  from different centers or acquisition protocols
                  differ considerably. This causes deep learning
                  models to fail silently. It is therefore crucial to
                  identify out-of-distribution (OOD) samples for which
                  the trained model is unsuitable. For models with a
                  self-supervised proxy task, we propose a simple
                  method to identify OOD samples that does not require
                  adapting the model architecture or access to a
                  separate OOD dataset during training. As the
                  performance of self-supervised tasks can be assessed
                  without ground truth information, it indicates
                  during test time when a sample differs from the
                  training distribution. The proposed method combines
                  a voxel-wise uncertainty estimate with the
                  self-supervision information. Our approach is
                  validated across three CMR datasets and two
                  different proxy tasks. We find that it is more
                  effective at detecting OOD samples than
                  state-of-the-art post-hoc OOD detection and
                  uncertainty estimation approaches.},
  author =	 {Gonzalez, Camila and Mukhopadhyay, Anirban},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {205--218},
  title =	 {Self-supervised Out-of-distribution Detection for
                  Cardiac {CMR} Segmentation},
  openreview =	 {E5CpgfwHBoC},
}

@inproceedings{gruening21,
  abstract =	 {With in-line holography, it is possible to record
                  biological cells over time in a three-dimensional
                  hydrogel without the need for staining, providing
                  the capability of observing cell behavior in a
                  minimally invasive manner. However, this setup
                  currently requires computationally intensive
                  image-reconstruction algorithms to determine the
                  required cell statistics. In this work, we directly
                  extract cell positions from the holographic data by
                  using deep neural networks and thus avoid several
                  reconstruction steps. We show that our method is
                  capable of substantially decreasing the time needed
                  to extract information from the raw data without
                  loss in quality.},
  author =	 {Gruening, Philipp and Nette, Falk and Heldt, Noah
                  and de Souza, Ana Cristina Guerra and Barth,
                  Erhardt},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {219--227},
  title =	 {Direct Inference of Cell Positions using Lens-Free
                  Microscopy and Deep Learning},
  openreview =	 {2fpsTsvCgc0},
}

@inproceedings{gupta21,
  abstract =	 {Ensuring the privacy of research participants is
                  vital, even more so in healthcare environments. Deep
                  learning approaches to neuroimaging require large
                  datasets, and this often necessitates sharing data
                  between multiple sites, which is antithetical to the
                  privacy objectives. Federated learning is a commonly
                  proposed solution to this problem. It circumvents
                  the need for data sharing by sharing parameters
                  during the training process. However, we demonstrate
                  that allowing access to parameters may leak private
                  information even if data is never directly
                  shared. In particular, we show that it is possible
                  to infer if a sample was used to train the model
                  given only access to the model prediction
                  (black-box) or access to the model itself
                  (white-box) and some leaked samples from the
                  training data distribution. Such attacks are
                  commonly referred to as \textit{Membership Inference
                  attacks}. We show realistic Membership Inference
                  attacks on deep learning models trained for 3D
                  neuroimaging tasks in a centralized as well as
                  decentralized setup. We demonstrate feasible attacks
                  on brain age prediction models (deep learning models
                  that predict a person's age from their brain MRI
                  scan). We correctly identified whether an MRI scan
                  was used in model training with a 60\% to over 80\%
                  success rate depending on model complexity and
                  security assumptions.},
  author =	 {Gupta, Umang and Stripelis, Dimitris and Lam,
                  Pradeep K. and Thompson, Paul and Ambite, Jose Luis
                  and Steeg, Greg Ver},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {228--251},
  title =	 {Membership Inference Attacks on Deep Regression
                  Models for Neuroimaging},
  openreview =	 {8lL_y9n-CV},
}

@inproceedings{hagenah21,
  abtract =	 {In personalized prosthesis shaping, the desired
                  shape remains typically unknown and has to be
                  estimated based on the individual pathological
                  shape. This estimation is also called pseudo healthy
                  synthesis. One example application is the
                  personalization of aortic root prostheses during
                  valve-sparing aortic root surgery. Even though
                  several methods for pseudohealthy synthesis were
                  proposed during the last years, it might not always
                  be necessary to taylor a completely individual and
                  unique prosthesis for each and every patient as this
                  introduces high costs and regulatory issues. Another
                  option is to identify a set of prosthesis types that
                  represents all natural healthy shapes in an adequate
                  way. Then, the pseudohealthy synthesis problem
                  becomes a classification problem, aiming on
                  predicting the optimal prosthesis out of the set of
                  candidates given a pathological shape. In this work,
                  we present a fully automized workflow of
                  unsupervised shape typification and type
                  classification based on pathological data for the
                  example of personalizing aortic root prostheses
                  shapes. We provide a proof-of-concept study on an
                  ex-vivo porcine data set, including a thorough
                  evaluation of the model's hyperparameters and the
                  number of identified shape types. Our study lies the
                  groundwork for a new branch of personalized
                  prosthesis shaping with a high potential of
                  translation to clinical application: Discrete
                  Pseudohealthy Synthesis.},
  author =	 {Hagenah, Jannis and Ernst, Floris},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {252--267},
  title =	 {Discrete Pseudohealthy Synthesis: Aortic Root Shape
                  Typification and Type Classification with
                  Pathological Prior},
  openreview =	 {Fqmbjvawgt},
}

@inproceedings{he21,
  abstract =	 {Weakly supervised segmentation is an important
                  problem in medical image analysis due to the high
                  cost of pixelwise annotation. Prior methods, while
                  often focusing on weak labels of 2D images, exploit
                  few structural cues of volumetric medical images. To
                  address this, we propose a novel weakly-supervised
                  segmentation strategy capable of better capturing 3D
                  shape prior in both model prediction and
                  learning. Our main idea is to extract a self-taught
                  shape representation by leveraging weak labels, and
                  then integrate this representation into segmentation
                  prediction for shape refinement. To this end, we
                  design a deep network consisting of a segmentation
                  module and a shape denoising module, which are
                  trained by an iterative learning strategy. Moreover,
                  we introduce a weak annotation scheme with a hybrid
                  label design for volumetric images, which improves
                  model learning without increasing the overall
                  annotation cost. The empirical experiments show that
                  our approach outperforms existing SOTA strategies on
                  three organ segmentation benchmarks with distinctive
                  shape properties. Notably, we can achieve strong
                  performance with even 10\% labeled slices, which is
                  significantly superior to other methods.},
  author =	 {He, Qian and Li, Shuailin and He, Xuming},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {268--285},
  title =	 {Weakly Supervised Volumetric Segmentation via
                  Self-taught Shape Denoising Model},
  openreview =	 {Koyg3kvH-Mq},
}

@inproceedings{heer21,
  abstract =	 {Deep unsupervised generative models are regarded as
                  a promising alternative to supervised counterparts
                  in the field of MRI-based lesion detection. They
                  denote a principled approach for detecting unseen
                  types of anomalies without relying on large amounts
                  of expensive ground truth annotations. To this end,
                  deep generative models are trained exclusively on
                  data from healthy patients and detect lesions as
                  Out-of-Distribution (OOD) data at test time
                  (i.e. low likelihood). While this is a promising way
                  of bypassing the need for costly annotations, this
                  work demonstrates that it also renders this widely
                  used unsupervised anomaly detection approach
                  particularly vulnerable to non-lesion-based OOD data
                  (e.g. data from different sensors). Since models are
                  likely to be exposed to such OOD data in production,
                  it is crucial to employ safety mechanisms to filter
                  for such samples and run inference only on input for
                  which the model is able to provide reliable
                  results. We first show extensively that
                  conventional, unsupervised anomaly detection
                  mechanisms fail when being presented with true OOD
                  data. Secondly, we apply prior knowledge to
                  disentangle lesion-based OOD from their
                  non-lesion-based counterparts.},
  author =	 {Heer, Matth{\"a}us and Postels, Janis and Chen,
                  Xiaoran and Konukoglu, Ender and Albarqouni, Shadi},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {286--300},
  title =	 {The {OOD} Blind Spot of Unsupervised Anomaly
                  Detection},
  openreview =	 {ZDD2TbZn7X1},
}

@inproceedings{hemati21,
  abstract =	 {Digital pathology has enabled us to capture, store
                  and analyze scanned biopsy samples as digital
                  images. Recent advances in deep learning are
                  contributing to computational pathology to improve
                  diagnosis and treatment. However, considering
                  challenges inherent to whole slide images (WSIs), it
                  is not easy to employ deep learning in digital
                  pathology. More importantly, computational
                  bottlenecks induced by the gigapixel WSIs make it
                  difficult to use deep learning for end-to-end image
                  representation. To mitigate this challenge, many
                  patch-based approaches have been proposed. Although
                  patching WSIs enables us to use deep learning, we
                  end up with a bag of patches or set representation
                  which makes downstream tasks non-trivial. More
                  importantly, considering set representation per WSI,
                  it is not clear how one can obtain similarity
                  between two WSIs (sets) for tasks like image search
                  matching. To address this challenge, we propose a
                  neural network based on Convolutions Neural Network
                  (CNN) and Deep Sets to learn one permutation
                  invariant vector representation per WSI in an
                  end-to-end manner. Considering available labels at
                  the WSI level - namely, primary site and cancer
                  subtypes - we train the proposed network in a
                  multi-label setting to encode both primary site and
                  diagnosis. Having in mind that every primary site
                  has its own specific cancer subtypes, we propose to
                  use the predicted label for the primary site to
                  recognize the cancer subtype. The proposed
                  architecture is used for transfer learning of WSIs
                  and validated two different tasks, i.e., search and
                  classification. The results show that the proposed
                  architecture can be used to obtain WSI
                  representations that achieve better performance both
                  in terms of retrieval performance and search time
                  against \emph{Yottixel}, a recently developed search
                  engine for pathology images. Further, the model
                  achieved competitive performance against the
                  state-of-art in lung cancer classification.},
  author =	 {Hemati, Sobhan and Kalra, Shivam and Meaney, Cameron
                  and Babaie, Morteza and Ghodsi, Ali and Tizhoosh,
                  Hamid},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {301--311},
  title =	 {{CNN} and Deep Sets for End-to-End Whole Slide Image
                  Representation Learning},
  openreview =	 {BX0kKB1zB1Q},
}

@inproceedings{hering21,
  abstract =	 {In follow-up CT examinations of cancer patients,
                  therapy success is evaluated by estimating the
                  change in tumor size. This process is time-consuming
                  and error-prone. We present a pipeline that
                  automates the segmentation and measurement of
                  matching lesions, given a point annotation in the
                  baseline lesion. First, a region around the point
                  annotation is extracted, in which a
                  deep-learning-based segmentation of the lesion is
                  performed. Afterward, a registration algorithm finds
                  the corresponding image region in the follow-up scan
                  and the convolutional neural network segments
                  lesions inside this region. In the final step, the
                  corresponding lesion is selected. We evaluate our
                  pipeline on clinical follow-up data comprising 125
                  soft-tissue lesions from 43 patients with metastatic
                  melanoma. Our pipeline succeeded for $96\%$ of the
                  baseline and $80\%$ of the follow-up lesions,
                  showing that we have laid the foundation for an
                  efficient quantitative follow-up assessment in
                  clinical routine.},
  author =	 {Hering, Alessa and Peisen, Felix and Amaral, Teresa
                  and Gatidis, Sergios and Eigentler, Thomas and
                  Othman, Ahmed and Moltz, Jan Hendrik},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {312--326},
  title =	 {Whole-Body Soft-Tissue Lesion Tracking and
                  Segmentation in Longitudinal {CT} Imaging Studies},
  openreview =	 {hzbuHGhU02Z},
}

@inproceedings{hu21,
  abstract =	 {Artifacts, blur, and noise are the common
                  distortions degrading MRI images during the
                  acquisition process, and deep neural networks have
                  been demonstrated to help in improving image
                  quality. To well exploit global structural
                  information and self-similarity details, we propose
                  a novel MR image enhancement network, named Feedback
                  Graph Attention Convolutional Network (FB-GACN). As
                  a key innovation, we consider the global structure
                  of an image by building a graph network from image
                  sub-regions that we consider to be node features,
                  linking them non-locally according to their
                  similarity. The proposed model consists of three
                  main parts: 1) The parallel graph similarity branch
                  and content branch, where the graph similarity
                  branch aims at exploiting the similarity and
                  symmetry across different image sub-regions in
                  low-resolution feature space and provides additional
                  priors for the content branch to enhance texture
                  details. 2) A feedback mechanism with a recurrent
                  structure to refine low-level representations with
                  high-level information and generate powerful
                  high-level texture details by handling the feedback
                  connections.  3) A reconstruction to remove the
                  artifacts and recover super-resolution images by
                  using the estimated sub-region self-similarity
                  priors obtained from the graph similarity branch. We
                  evaluate our method on two image enhancement tasks:
                  i) cross-protocol super resolution of diffusion MRI;
                  ii) artifact removal of FLAIR MR
                  images. Experimental results demonstrate that the
                  proposed algorithm outperforms the state-of-the-art
                  methods.},
  author =	 {Hu, Xiaobin and Yan, Yanyang and Ren, Wenqi and Li,
                  Hongwei and Bayat, Amirhossein and Zhao, Yu and
                  Menze, Bjoern},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {327--337},
  title =	 {Feedback Graph Attention Convolutional Network for
                  {MR} Images Enhancement by Exploring Self-Similarity
                  Features},
  openreview =	 {k1BSWQqHoMV},
}

@inproceedings{kanavati21,
  abstract =	 {Transfer learning from ImageNet is the go-to
                  approach when applying deep learning to medical
                  images. The approach is either to fine-tune a
                  pre-trained model or use it as a feature extractor.
                  Most modern architecture contain batch normalisation
                  layers, and fine-tuning a model with such layers
                  requires taking a few precautions as they consist of
                  trainable and non-trainable weights and have two
                  operating modes: training and inference. Attention
                  is primarily given to the non-trainable weights used
                  during inference, as they are the primary source of
                  unexpected behaviour or degradation in performance
                  during transfer learning. It is typically
                  recommended to fine-tune the model with the batch
                  normalisation layers kept in inference mode during
                  both training and inference. In this paper, we pay
                  closer attention instead to the trainable weights of
                  the batch normalisation layers, and we explore their
                  expressive influence in the context of transfer
                  learning.  We find that only fine-tuning the
                  trainable weights (scale and centre) of the batch
                  normalisation layers leads to similar performance as
                  to fine-tuning all of the weights, with the added
                  benefit of faster convergence. We demonstrate this
                  on a variety of seven publicly available medical
                  imaging datasets, using four different model
                  architectures.},
  author =	 {Kanavati, Fahdi and Tsuneki, Masayuki},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {338--353},
  title =	 {Partial transfusion: on the expressive influence of
                  trainable batch norm parameters for transfer
                  learning},
  openreview =	 {TjwDWRdfZpg},
}

@inproceedings{kervadec21,
  abstract =	 {Standard losses for training deep segmentation
                  networks could be seen as individual classifications
                  of pixels, instead of supervising the global shape
                  of the predicted segmentations. While effective,
                  they require exact knowledge of the label of each
                  pixel in an image. This study investigates how
                  effective global geometric shape descriptors could
                  be, when used on their own as segmentation losses
                  for training deep networks. Not only interesting
                  theoretically, there exist deeper motivations to
                  posing segmentation problems as a reconstruction of
                  shape descriptors: First, annotations to obtain
                  approximations of low-order shape moments could be
                  much less cumbersome than their full-mask
                  counterparts, and anatomical priors could be readily
                  encoded into invariant shape descriptions, which
                  might alleviate the annotation burden. Also, some
                  shape descriptors could be readily used to
                  ``encode'' biomarkers, leading to better
                  interpretability. Finally, and most importantly, we
                  hypothesize that, given a task, certain shape
                  descriptions might be invariant across image
                  acquisition protocols/modalities and subject
                  populations, which might open interesting research
                  avenues for generalization in medical image
                  segmentation. We introduce and formulate a few shape
                  descriptors in the context of deep segmentation, and
                  evaluate their potential as stand-alone losses on
                  two different, challenging tasks. Inspired by recent
                  works in constrained optimization for deep networks,
                  we propose a way to use those descriptors to
                  supervise segmentation, without any pixel-level
                  label. Very surprisingly, as little as 4 descriptors
                  values per class can approach the performance of a
                  segmentation mask with 65k individual discrete
                  labels. We also found that shape descriptors can be
                  a valid way to encode anatomical priors about the
                  task, enabling to leverage expert knowledge without
                  requiring additional annotations. Our implementation
                  is publicly available and can be easily extended.},
  author =	 {Kervadec, Hoel and Bahig, Houda and
                  Letourneau-Guillon, Laurent and Dolz, Jose and Ayed,
                  Ismail Ben},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {354--368},
  title =	 {Beyond pixel-wise supervision for segmentation: A
                  few global shape descriptors might be surprisingly
                  good!},
  openreview =	 {nqe6e0oJ_fL},
}

@inproceedings{kist21,
  abstract =	 {Images offer a two-dimensional (2D) representation
                  of a three-dimensional (3D) environment. However, in
                  many biomedical tasks, a 3D view is crucial for
                  diagnosis. Projecting structured light, such as a
                  regular laser grid, onto the surface of interest
                  allows to reconstruct its 3D structure. For
                  reconstruction, it is crucial to correctly identify
                  and assign each laser ray to its respective position
                  in the laser grid. Current methods for this task use
                  semi-automatic, yet highly manual
                  annotations. Hence, a fully automatic, reliable
                  method is desired. Here, we show that this
                  assignment can be approached as an image
                  registration. We first separate the laser rays from
                  the background using semantic segmentation. We found
                  that registration of the extracted laser rays
                  directly to the fixed laser grid image fails, when
                  we use state-of-the-art intensity-based image
                  registration techniques, such as ANTs. Using our
                  feature-based custom loss and a deep neural network,
                  we are able to use a U-Net-like architecture to
                  compute deformation fields to successfully register
                  the laser rays onto the fixed image accompanied with
                  a custom post-processing sorting step. Using
                  synthetic data, we show that the network is in
                  general able to learn affine and non-linear
                  transformations. Our method is also robust to
                  missing or occluded rays. Using an ex vivo dataset,
                  we achieved an registration accuracy of 91\%. In
                  summary, we provide a new platform to perform
                  feature-based registration and showcase this on a
                  biomedical dataset. In future, we will evaluate
                  different architectural designs and more complex
                  datasets.},
  author =	 {Kist, Andreas M and Zilker, Julian and
                  D{\"o}llinger, Michael and Semmler, Marion},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {369--383},
  title =	 {Feature-based image registration in structured light
                  endoscopy},
  openreview =	 {MzC8X6cMF2r},
}

@inproceedings{konwer21,
  abstract =	 {Automated analyses of chest imaging in Coronavirus
                  Disease 2019 (COVID-19) have largely focused on a
                  single timepoint, usually at disease presentation,
                  and have not explicitly taken into account temporal
                  disease manifestations. We present a deep
                  learning-based approach for prediction of imaging
                  progression from serial chest radiographs (CXRs) of
                  COVID-19 patients. Our method first utilizes
                  convolutional neural networks (CNNs) for feature
                  extraction from patches within the concerned lung
                  zone, and also from neighboring areas to enhance the
                  contextual phenotypic information. The framework
                  further incorporates two distinct spatio-temporal
                  Long Short Term Memory (LSTM) modules for effective
                  predictions. The first LSTM module captures spatial
                  dependencies between patches and the second exploits
                  the temporal context of sequential CXR scans. The
                  resulting network focuses on critical image regions
                  that provide relevant information for learning the
                  progression of lung infiltrates without the explicit
                  need for infiltrate segmentation. The second LSTM
                  provides an encoded context vector used as an input
                  to a decoder module to predict future severity
                  grades. Our novel multi-institutional dataset
                  comprises sequential CXR scans from N=100
                  patients. Specifically, our framework predicts
                  zone-wise disease severity for a patient on the last
                  day by learning representations from the previous
                  temporal CXRs. We design two baseline approaches -
                  one using fine-tuned VGG-16 features and the other
                  using radiomic descriptors. Experimental results
                  demonstrate that our proposed approach outperforms
                  both baselines in average accuracy by 10.33\% and
                  12.16\%, respectively, in predicting COVID-19
                  progression severity.},
  author =	 {Konwer, Aishik and Bae, Joseph and Singh, Gagandeep
                  and Gattu, Rishabh and Ali, Syed and Green, Jeremy
                  and Phatak, Tej and Gupta, Amit and Chen, Chao and
                  Saltz, Joel and Prasanna, Prateek},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {384--398},
  title =	 {Predicting {COVID}-19 Lung Infiltrate Progression on
                  Chest Radiographs Using Spatio-temporal {LSTM} based
                  Encoder-Decoder Network},
  openreview =	 {96BhL_MERil},
}

@inproceedings{lalit21,
  abstract =	 {Automatic detection and segmentation of objects in
                  2D and 3D microscopy data is important for countless
                  biomedical applications. In the natural image
                  domain, spatial embedding-based instance
                  segmentation methods are known to yield high-quality
                  results, but their utility for segmenting microscopy
                  data is currently little researched. Here we
                  introduce EmbedSeg, an embedding-based instance
                  segmentation method which outperforms existing
                  state-of-the-art baselines on 2D as well as 3D
                  microscopy datasets. Additionally, we show that
                  EmbedSeg has a GPU memory footprint small enough to
                  train even on laptop GPUs, making it accessible to
                  virtually everyone. Finally, we introduce four new
                  3D microscopy datasets, which we make publicly
                  available alongside ground truth training
                  labels. Our open-source implementation is available
                  at https://github.com/juglab/EmbedSeg.},
  author =	 {Lalit, Manan and Tomancak, Pavel and Jug, Florian},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {399--415},
  title =	 {Embedding-based Instance Segmentation in Microscopy},
  openreview =	 {JM6GuFGayL5},
}

@inproceedings{lemay21,
  abstract =	 {Medical images are often accompanied by metadata
                  describing the image (vendor, acquisition
                  parameters) and the patient (disease type or
                  severity, demographics, genomics). This metadata is
                  usually disregarded by image segmentation
                  methods. In this work, we adapt a linear
                  conditioning method called FiLM (Feature-wise Linear
                  Modulation) for image segmentation tasks. This FiLM
                  adaptation enables integrating metadata into
                  segmentation models for better performance. We
                  observed an average Dice score increase of 5.1\% on
                  spinal cord tumor segmentation when incorporating
                  the tumor type with FiLM. The metadata modulates the
                  segmentation process through low-cost affine
                  transformations applied on feature maps which can be
                  included in any neural network's
                  architecture. Additionally, we assess the relevance
                  of segmentation FiLM layers for tackling common
                  challenges in medical imaging: training with limited
                  or unbalanced number of annotated data, multi-class
                  training with missing segmentations, and model
                  adaptation to multiple tasks. Our results
                  demonstrated the following benefits of FiLM for
                  segmentation: FiLMed U-Net was robust to missing
                  labels and reached higher Dice scores with few
                  labels (up to 16.7\%) compared to single-task
                  U-Net. The code is open-source and available at
                  www.ivadomed.org.},
  author =	 {Lemay, Andreanne and Gros, Charley and Vincent,
                  Olivier and Liu, Yaou and Cohen, Joseph Paul and
                  Cohen-Adad, Julien},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {416--430},
  title =	 {Benefits of Linear Conditioning for Segmentation
                  using Metadata},
  openreview =	 {fa176bQAbr},
}

@inproceedings{liu21a,
  abtract =	 {Quantitative susceptibility mapping (QSM) is a
                  magnetic resonance imaging (MRI) technique that
                  estimates magnetic susceptibility of tissue from MR
                  phase measurements. Recently, several supervised
                  deep learning (DL) techniques have demonstrated
                  impressive performance in solving the challenging
                  ill-posed field-to-source inverse QSM reconstruction
                  problem. To address the lack of the inherent
                  non-existent ground-truth QSM references, a
                  model-based method was recently proposed using the
                  well-established physical model. However, it fails
                  to perform well at the regions with large
                  susceptibility variations. Here, we proposed uQSM+
                  with data augmentation techniques to improve the
                  model-based learning. The proposed method was
                  evaluated on a multi-orientation QSM datasets and
                  2019 QSM reconstruction challenge
                  datasets. Quantitative and qualitative evaluation
                  showed that uQSM+ and zero-shot uQSM+ was capable of
                  reconstructing high quality QSM. The code is
                  available at
                  \inkhttps{https://github.com/juana313/uQSM-plus}.},
  author =	 {Liu, Juan},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {431--450},
  title =	 {Improved model-based deep learning for quantitative
                  susceptibility mapping},
  openreview =	 {Y7koM_09Cme},
}

@inproceedings{liu21b,
  abstract =	 {In the last few years, deep learning classifiers
                  have shown promising results in image-based medical
                  diagnosis. However, interpreting the outputs of
                  these models remains a challenge. In cancer
                  diagnosis, interpretability can be achieved by
                  localizing the region of the input image responsible
                  for the output, i.e. the location of a
                  lesion. Alternatively, segmentation or detection
                  models can be trained with pixel-wise annotations
                  indicating the locations of malignant
                  lesions. Unfortunately, acquiring such labels is
                  labor-intensive and requires medical expertise. To
                  overcome this difficulty, weakly-supervised
                  localization can be utilized. These methods allow
                  neural network classifiers to output saliency maps
                  highlighting the regions of the input most relevant
                  to the classification task (e.g. malignant lesions
                  in mammograms) using only image-level labels
                  (e.g. whether the patient has cancer or not) during
                  training. When applied to high-resolution images,
                  existing methods produce low-resolution saliency
                  maps. This is problematic in applications in which
                  suspicious lesions are small in relation to the
                  image size. In this work, we introduce a novel
                  neural network architecture to perform
                  weakly-supervised segmentation of high-resolution
                  images. The proposed model selects regions of
                  interest via coarse-level localization, and then
                  performs fine-grained segmentation of those
                  regions. We apply this model to breast cancer
                  diagnosis with screening mammography, and validate
                  it on a large clinically-realistic dataset. Measured
                  by Dice similarity score, our approach outperforms
                  existing methods by a large margin in terms of
                  localization performance of benign and malignant
                  lesions, relatively improving the performance by
                  39.6\% and 20.0\%, respectively. Code and the
                  weights of some of the models are available at
                  https://github.com/nyukat/GLAM},
  author =	 {Liu, Kangning and Shen, Yiqiu and Wu, Nan and
                  Ch{\l}{\k{e}}dowski, Jakub Piotr and
                  Fernandez-Granda, Carlos and Geras, Krzysztof J.},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {451--472},
  title =	 {Weakly-supervised High-resolution Segmentation of
                  Mammography Images for Breast Cancer Diagnosis},
  openreview =	 {nBT8eNF7aXr},
}

@inproceedings{maheshwari21,
  abtract =	 {Accurate segmentation of volumetric scans like MRI
                  and CT scans is highly demanded for surgery planning
                  in clinical practice, quantitative analysis, and
                  identification of disease. However, accurate
                  segmentation is challenging because of the irregular
                  shape of given organ and large variation in
                  appearances across the slices. In such problems, 3D
                  features are desired in nature which can be
                  extracted using 3D convolutional neural network
                  (CNN). However, 3D CNN is compute and memory
                  intensive to implement due to large number of
                  parameters and can easily over fit, especially in
                  medical imaging where training data is limited. In
                  order to address these problems, we propose a
                  distillation-based depth shift module (Distill
                  DSM). It is designed to enable 2D convolutions to
                  make use of information from neighbouring frames
                  more efficiently. Specifically, in each layer of the
                  network, Distill DSM learns to extract information
                  from a part of the channels and shares it with
                  neighbouring slices, thus facilitating information
                  exchange among neighbouring slices. This approach
                  can be incorporated with any 2D CNN model to enable
                  it to use information across the slices with
                  introducing very few extra learn-able parameters. We
                  have evaluated our model on BRATS 2020, heart,
                  hippocampus, pancreas and prostate dataset. Our
                  model achieves better performance than 3D CNN for
                  heart and prostate datasets and comparable
                  performance on BRATS 2020, pancreas and hippocampus
                  dataset with simply 28\% of parameters compared to
                  3D CNN model.},
  author =	 {Maheshwari, Harsh and Goel, Vidit and Sethuraman,
                  Ramanathan and Sheet, Debdoot},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {473--483},
  title =	 {Distill {DSM}: Computationally efficient method for
                  segmentation of medical imaging volumes},
  openreview =	 {_n48l6YKc6d},
}

@inproceedings{mairhoefer21,
  abstract =	 {The quality of radiographs is of major importance
                  for diagnosis and treatment planning. While most
                  research regarding automated radiograph quality
                  assessment uses technical features such as noise or
                  contrast, we propose to use anatomical structures as
                  more appropriate features. We show that based on
                  such anatomical features, a modular deep-learning
                  framework can serve as a quality control mechanism
                  for the diagnostic quality of ankle radiographs. For
                  evaluation, a dataset consisting of 950 ankle
                  radiographs was collected and their quality was
                  labeled by radiologists. We obtain an average
                  accuracy of 94.1\%, which is better than the expert
                  radiologists are on average.},
  author =	 {Mairh{\"o}fer, Dominik and Laufer, Manuel and Simon,
                  Paul Martin and Sieren, Malte and Bischof, Arpad and
                  K{\"a}ster, Thomas and Barth, Erhardt and
                  Barkhausen, J{\"o}rg and Martinetz, Thomas},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {484--496},
  title =	 {An {AI}-based Framework for Diagnostic Quality
                  Assessment of Ankle Radiographs},
  openreview =	 {bj04hJss\_xZ},
}

@inproceedings{mouches21,
  abstract =	 {Age-related morphological brain changes are known to
                  be different in healthy and disease-affected
                  aging. Biological brain age estimation from MRI
                  scans is a common way to quantify this effect
                  whereas differences between biological and
                  chronological age indicate degenerative
                  processes. The ability to visualize and analyze the
                  morphological age-related changes in the image space
                  directly is essential to improve the understanding
                  of brain aging. In this work, we propose a novel
                  deep learning based approach to unify biological
                  brain age estimation and age-conditioned template
                  creation in a single, consistent model. We achieve
                  this by developing a deterministic autoencoder that
                  successfully disentangles age-related morphological
                  changes and subject-specific variations. This allows
                  its use as a brain age regressor as well as a
                  generative brain aging model. The proposed approach
                  demonstrates accurate biological brain age
                  prediction, and realistic generation of
                  age-conditioned brain templates and simulated
                  age-specific brain images when applied to a database
                  of more than 2000 subjects.},
  author =	 {Mouches, Pauline and Wilms, Matthias and Rajashekar,
                  Deepthi and Langner, Sonke and Forkert, Nils},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {497--506},
  title =	 {Unifying Brain Age Prediction and Age-Conditioned
                  Template Generation with a Deterministic
                  Autoencoder},
  openreview =	 {9ClUQ2ELJap},
}

@inproceedings{muhamedrahimov21,
  abstract =	 {In classification, categories are typically treated
                  as independent of one-another. In many problems,
                  however, this neglects the natural relations that
                  exist between categories, which are often dictated
                  by an underlying biological or physical process. In
                  this work, we propose novel formulations of the
                  classification problem, aimed at reintroducing class
                  relations into the training process. We demonstrate
                  the benefit of these approaches for the
                  classification of intravenous contrast enhancement
                  phase in CT images, an important task in the medical
                  imaging domain. First, we propose manual ways
                  reintroduce knowledge about problem-specific
                  interclass relations into the training
                  process. Second, we propose a general approach to
                  jointly learn categorical label representations that
                  can implicitly encode natural interclass relations,
                  alleviating the need for strong prior assumptions or
                  knowledge. We show that these improvements are most
                  significant for smaller training sets, typical in
                  the medical imaging domain where access to large
                  amounts of labelled data is often not trivial.},
  author =	 {Muhamedrahimov, Raouf and Bar, Amir and
                  Akselrod-Ballin, Ayelet},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {507--519},
  title =	 {Learning Interclass Relations for Intravenous
                  Contrast Phase Classification in {CT}},
  openreview =	 {B01pd5ot0w6},
}

@inproceedings{muhammad21,
  abstract =	 {Histopathology-based survival modelling has two
                  major hurdles. Firstly, a well-performing survival
                  model has minimal clinical application if it does
                  not contribute to the stratification of a cancer
                  patient cohort into different risk groups,
                  preferably driven by histologic morphologies. In the
                  clinical setting, individuals are not given specific
                  prognostic predictions, but are rather predicted to
                  lie within a risk group which has a general survival
                  trend. Thus, It is imperative that a survival model
                  produces well-stratified risk groups. Secondly,
                  until now, survival modelling was done in a
                  two-stage approach (encoding and
                  aggregation). EPIC-Survival bridges encoding and
                  aggregation into an end-to-end survival modelling
                  approach, while introducing stratification boosting
                  to encourage the model to not only optimize ranking,
                  but also to discriminate between risk groups. In
                  this study we show that EPIC-Survival performs
                  better than other approaches in modelling
                  intrahepatic cholangiocarcinoma (ICC), a
                  historically difficult cancer to model. We found
                  that stratification boosting further improves model
                  performance and helps identify specific histologic
                  differences, not commonly sought out in ICC.},
  author =	 {Muhammad, Hassan and Xie, Chensu and Sigel, Carlie S
                  and Doukas, Michael and Alpert, Lindsay and Simpson,
                  Amber Lea and Fuchs, Thomas J},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {520--531},
  title =	 {{EPIC}-Survival: End-to-end Part Inferred Clustering
                  for Survival Analysis, with Prognostic
                  Stratification Boosting},
  openreview =	 {JSSwHS_GU63},
}

@inproceedings{neimark21,
  abstract =	 {Prior work demonstrated the ability of machine
                  learning to automatically recognize surgical
                  workflow steps from videos. However, these studies
                  focused on only a single type of procedure. In this
                  work, we analyze, for the first time, surgical step
                  recognition on four different laparoscopic
                  surgeries: Cholecystectomy, Right Hemicolectomy,
                  Sleeve Gastrectomy, and Appendectomy. Inspired by
                  the traditional apprenticeship model, in which
                  surgical training is based on the Halstedian method,
                  we paraphrase the ``see one, do one, teach one''
                  approach for the surgical intelligence domain as
                  ``train one, classify one, teach one''. In machine
                  learning, this approach is often referred to as
                  transfer learning. To analyze the impact of transfer
                  learning across different laparoscopic procedures,
                  we explore various time-series architectures and
                  examine their performance on each target domain. We
                  introduce a new architecture, the Time-Series
                  Adaptation Network (TSAN), an architecture optimized
                  for transfer learning of surgical step recognition,
                  and we show how TSAN can be pre-trained using
                  self-supervised learning on a Sequence Sorting
                  task. Such pre-training enables TSAN to learn
                  workflow steps of a new laparoscopic procedure type
                  from only a small number of labeled samples from the
                  target procedure. Our proposed architecture leads to
                  better performance compared to other possible
                  architectures, reaching over 90\% accuracy when
                  transferring from laparoscopic Cholecystectomy to
                  the other three procedure types.},
  author =	 {Neimark, Daniel and Bar, Omri and Zohar, Maya and
                  Hager, Gregory D. and Asselmann, Dotan},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {532--544},
  title =	 {{\textquotedblleft}Train one, Classify one, Teach
                  one{\textquotedblright} - Cross-surgery transfer
                  learning for surgical step recognition},
  openreview =	 {cTB4Qz3RzCl},
}

@inproceedings{nguyen21,
  abstract =	 {Deep learning in medical image analysis often
                  requires an extensive amount of high-quality labeled
                  data for training to achieve Human-level
                  accuracy. We propose Gist-set Online Active Learning
                  (GOAL), a novel solution for limited high-quality
                  labeled data in medical imaging analysis. Our
                  approach advances the existing active learning
                  methods in three aspects. Firstly, we improve the
                  classification performance with fewer manual
                  annotations by presenting a sample selection
                  strategy called gist set selection. Secondly, unlike
                  traditional methods focusing only on random
                  uncertain samples of low prediction confidence, we
                  propose a new method in which only informative
                  uncertain samples are selected for human
                  annotation. Thirdly, we propose an application of
                  online learning where high-confidence samples are
                  automatically selected, iteratively assigned, and
                  pseudo-labels are updated. We validated our approach
                  on two private and one public dataset. The
                  experimental results show that, by applying GOAL, we
                  can reduce required labeled data up to 88\% while
                  maintaining the same F1 scores compared to the
                  models trained on full datasets},
  author =	 {Nguyen, Chanh and Huynh, Minh Thanh and Tran, Minh
                  Quan and Nguyen, Ngoc Hoang and Jain, Mudit and Ngo,
                  Van Doan and Vo, Tan Duc and Bui, Trung and Truong,
                  Steven Quoc Hung},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {545--553},
  title =	 {{GOAL}: Gist-set Online Active Learning for
                  Efficient Chest X-ray Image Annotation},
  openreview =	 {boTEEpM8mu},
}

@inproceedings{olivier21,
  abstract =	 {In this paper, we propose a novel approach to
                  overcome the problem of imbalanced datasets for
                  object detection tasks, when the distribution is not
                  uniform over all classes. The general idea is to
                  compute a probability vector, encoding the
                  probability for each image to be fed to the network
                  during the training phase. This probability vector
                  is computed by solving some quadratic optimization
                  problem and ensures that all classes are seen with
                  similar frequency. We apply this method to a fetal
                  anatomies detection problem, and conduct a thorough
                  statistical analysis of the resulting performance to
                  show that it performs significantly better than two
                  baseline models: one with images sampled uniformly
                  and one implementing classical oversampling.},
  author =	 {Olivier, Antoine and Raynaud, Caroline},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {554--566},
  title =	 {Balanced sampling for an object detection problem -
                  application to fetal anatomies detection},
  openreview =	 {ZGvtypAfHiA},
}

@inproceedings{ozer21,
  abstract =	 {Medical image quality assessment is an important
                  aspect of image acquisition where poor-quality
                  images may lead to misdiagnosis. In addition, manual
                  labelling of image quality after the acquisition is
                  often tedious and can lead to some misleading
                  results. Despite much research on the automated
                  analysis of image quality for tackling this problem,
                  relatively little work has been done for the
                  explanation of the methodologies. In this work, we
                  propose an explainable image quality assessment
                  system and validate our idea on foreign objects in a
                  Chest X-Ray (Object-CXR) dataset. Our explainable
                  pipeline relies on NormGrad, an algorithm, which can
                  efficiently localize the image quality issues with
                  saliency maps of the classifier. We compare our
                  method with a range of saliency detection methods
                  and illustrate the superior performance of NormGrad
                  by obtaining a Pointing Game accuracy of 0.862 on
                  the test dataset of the Object-CXR dataset. We also
                  verify our findings through a qualitative analysis
                  by visualizing attention maps for foreign objects on
                  X-Ray images.},
  author =	 {Ozer, Caner and Oksuz, Ilkay},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {567--580},
  title =	 {Explainable Image Quality Analysis of Chest X-Rays},
  openreview =	 {ln797A8lAb0},
}

@inproceedings{philipp21,
  abstract =	 {Towards computer-assisted neurosurgery, robust
                  methods for instrument localization on neurosurgical
                  microscope video data are needed. Specifically for
                  neurosurgical data, challenges arise from visual
                  conditions such as strong blur and from an
                  unknowingly large variety of instrument types. For
                  neurosurgical domain, instrument localization
                  methods must generalize across different
                  sub-disciplines such as cranial tumor and aneurysm
                  surgeries which exhibit different visual
                  properties. We present and evaluate a methodology
                  towards robust instrument tip localization for
                  neurosurgical microscope data, formulated as coarse
                  saliency prediction. For our analysis, we build a
                  comprehensive dataset comprising in-the-wild data
                  from several neurosurgical sub-disciplines as well
                  as phantom surgeries. Comparing single stream
                  networks using either image or optical flow
                  information, we find complementary performance of
                  both networks. Plain optical flow enables better
                  cross-domain generalization, while the image-based
                  network performs better on surgeries from the
                  training domain. Based on these findings, we present
                  a two-stream architecture that fuses image and
                  optical flow information to utilize the
                  complementary performance of both. Being trained on
                  tumor surgeries, our architecture outperforms both
                  single stream networks and shows improved robustness
                  on data from different neurosurgical
                  sub-disciplines. From our findings, future work must
                  focus more on how to incorporate optical flow
                  information into fusion architectures to further
                  improve cross-domain generalization.},
  author =	 {Philipp, Markus and Alperovich, Anna and Gutt-Will,
                  Marielena and Mathis, Andrea and Saur, Stefan and
                  Raabe, Andreas and Mathis-Ullrich, Franziska},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {581--595},
  title =	 {Localizing Neurosurgical Instruments Across Domains
                  and in the Wild},
  openreview =	 {21m0dBCMdd},
}

@inproceedings{pinaya21,
  abstract =	 {Pathological brain appearances may be so
                  heterogeneous as to be intelligible only as
                  anomalies, defined by their deviation from normality
                  rather than any specific pathological
                  characteristic. Amongst the hardest tasks in medical
                  imaging, detecting such anomalies requires models of
                  the normal brain that combine compactness with the
                  expressivity of the complex, long-range interactions
                  that characterise its structural organisation. These
                  are requirements transformers have arguably greater
                  potential to satisfy than other current candidate
                  architectures, but their application has been
                  inhibited by their demands on data and computational
                  resource. Here we combine the latent representation
                  of vector quantised variational autoencoders with an
                  ensemble of autoregressive transformers to enable
                  unsupervised anomaly detection and segmentation
                  defined by deviation from healthy brain imaging
                  data, achievable at low computational cost, within
                  relative modest data regimes. We compare our method
                  to current state-of-the-art approaches across a
                  series of experiments involving synthetic and real
                  pathological lesions. On real lesions, we train our
                  models on 15,000 radiologically normal participants
                  from UK Biobank, and evaluate performance on four
                  different brain MR datasets with small vessel
                  disease, demyelinating lesions, and tumours. We
                  demonstrate superior anomaly detection performance
                  both image-wise and pixel-wise, achievable without
                  post-processing. These results draw attention to the
                  potential of transformers in this most challenging
                  of imaging tasks.},
  author =	 {Pinaya, Walter Hugo Lopez and Tudosiu, Petru-Daniel
                  and Gray, Robert and Rees, Geraint and Nachev,
                  Parashkev and Ourselin, S{\'e}bastien and Cardoso,
                  M. Jorge},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {596--617},
  title =	 {Unsupervised Brain Anomaly Detection and
                  Segmentation with Transformers},
  openreview =	 {Z1tlNqbCpp_},
}

@inproceedings{pirkl21,
  abstract =	 {Subject motion is one of the major challenges in
                  clinical routine MR imaging. Despite ongoing
                  research, motion correction has remained a complex
                  problem without a universal solution. In advanced
                  quantitative MR techniques, such as MR
                  Fingerprinting, motion does not only affect a single
                  image, like in single-contrast MRI, but disrupts the
                  entire temporal evolution of the magnetization and
                  causes parameter quantification errors due to a
                  mismatch between the acquired and simulated
                  signals. In this work, we present a deep
                  learning-empowered retrospective motion correction
                  for rapid 3D whole-brain multiparametric MRI based
                  on Quantitative Transient-state Imaging (QTI). We
                  propose a patch-based 3D multiscale convolutional
                  neural network (CNN) that learns the residual error,
                  i.e. after initial navigator-based correction,
                  between motion-affected quantitative T1, T2 and
                  proton density maps and their motion-free
                  counterparts. For efficient model training despite
                  limited data availability, we propose a
                  physics-informed simulation to apply continuous
                  motion-patterns to motion-free data. We evaluate the
                  performance of the residual CNN on 1.5T and 3T MRI
                  data of ten healthy volunteers. We analyze the
                  generalizability of the model when applied to real
                  clinical cases, including pediatric and adult
                  patients with large brain lesions. Our study
                  demonstrates that image quality can be significantly
                  improved after correcting for subject motion. This
                  has important implications in clinical setups where
                  large amounts of motion affected data must be
                  discarded.},
  author =	 {Pirkl, Carolin and Cencini, Matteo and Kurzawski,
                  Jan W. and Waldmannstetter, Diana and Li, Hongwei
                  and Sekuboyina, Anjany and Endt, Sebastian and
                  Peretti, Luca and Donatelli, Graziella and
                  Pasquariello, Rosa and Tosetti, Michela and
                  Costagli, Mauro and Buonincontri, Guido and Menzel,
                  Marion I. and Menze, Bjoern H.},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {618--632},
  title =	 {Residual learning for 3D motion corrected
                  quantitative {MRI}: Robust clinical T1, T2 and
                  proton density mapping},
  openreview =	 {hxgQM71AuRA},
}

@inproceedings{prieto21,
  abstract =	 {Chlamydia trachomatous is an infectious ocular
                  condition that can cause the eyelid to turn inward
                  so that one or more eyelashes touch the eyeball, a
                  condition call trachomatous trichiasis (TT), which
                  can lead to blindness. Community-based screeners are
                  used in rural areas to identify patients with TT,
                  who can then be referred for proper medical
                  care. Having automatic methods to detect TT will
                  reduce the amount of time required to train
                  screeners and improve accuracy of detection. This
                  paper proposes a method to automatically identify
                  regions of an eye and identify TT, using photographs
                  taken with smartphones in the field. The
                  attention-based gated deep learning networks in
                  combination with a regionidentification network can
                  identify TT with an accuracy of 91\%, sensitivity of
                  92\% and specificity of 87\%, showing that these
                  methods have the potential to be deployed in the
                  field.},
  author =	 {Prieto, Juan Carlos and Shah, Hina and Jones, Kasey
                  and Chew, Robert F and Kana, Hashiya M. and Weaver,
                  Jerusha and Flueckiger, Rebecca M. and McPherson,
                  Scott and Gower, Emily W.},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {633--644},
  title =	 {Image Sequence Generation and Analysis via {GRU} and
                  Attention for Trachomatous Trichiasis
                  Classification},
  openreview =	 {umb5xsy1-zS},
}

@inproceedings{qiu21,
  abstract =	 {We present a deep learning (DL) registration
                  framework for fast mono-modal and multi-modal image
                  registration using differentiable mutual information
                  and diffeomorphic B-spline free-form deformation
                  (FFD). Deep learning registration has been shown to
                  achieve competitive accuracy and significant
                  speedups from traditional iterative registration
                  methods. In this paper, we propose to use a B-spline
                  FFD parameterisation of Stationary Velocity Field
                  (SVF) to in DL registration in order to achieve
                  smooth diffeomorphic deformation while being
                  computationally-efficient. In contrast to most DL
                  registration methods which use intensity similarity
                  metrics that assume linear intensity relationship,
                  we apply a differentiable variant of a classic
                  similarity metric, mutual information, to achieve
                  robust mono-modal and multi-modal registration. We
                  carefully evaluated our proposed framework on mono-
                  and multi-modal registration using 3D brain MR
                  images and 2D cardiac MR images.},
  author =	 {Qiu, Huaqi and Qin, Chen and Schuh, Andreas and
                  Hammernik, Kerstin and Rueckert, Daniel},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {645--664},
  title =	 {Learning Diffeomorphic and Modality-invariant
                  Registration using B-splines},
  openreview =	 {eSI9Qh2DJhN},
}

@inproceedings{rajagopal21,
  abstract =	 {Fully-convolutional neural networks, such as the 2D
                  or 3D UNet, are now pervasive in medical imaging for
                  semantic segmentation, classification, image
                  denoising, domain translation, and
                  reconstruction. However, evaluation of UNet
                  performance, as with most CNNs, has mostly been
                  relegated to evaluation of a few performance metrics
                  (e.g. accuracy, IoU, SSIM, etc.) using the network's
                  final predictions, which provides little insight
                  into important issues such as dataset shift that
                  occur in clinical application. In this paper, we
                  propose techniques for understanding and visualizing
                  the generalization performance of UNets in image
                  classification and regression tasks, giving rise to
                  metrics that are indicative of performance on a
                  withheld test-set without the need for groundtruth
                  annotations.},
  author =	 {Rajagopal, Abhejit and Madala, Vamshi Chowdary and
                  Hope, Thomas A and Larson, Peder},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {665--681},
  title =	 {Understanding and Visualizing Generalization in
                  {UN}ets},
  openreview =	 {V-a5DJCh4Hk},
}

@inproceedings{sharma21,
  abstract =	 {In recent years, the availability of digitized Whole
                  Slide Images (WSIs) has enabled the use of deep
                  learning-based computer vision techniques for
                  automated disease diagnosis. However, WSIs present
                  unique computational and algorithmic
                  challenges. WSIs are gigapixel-sized ($\sim$100K
                  pixels), making them infeasible to be used directly
                  for training deep neural networks. Also, often only
                  slide-level labels are available for training as
                  detailed annotations are tedious and can be
                  time-consuming for experts. Approaches using
                  multiple-instance learning (MIL) frameworks have
                  been shown to overcome these challenges. Current
                  state-of-the-art approaches divide the learning
                  framework into two decoupled parts: a convolutional
                  neural network (CNN) for encoding the patches
                  followed by an independent aggregation approach for
                  slide-level prediction. In this approach, the
                  aggregation step has no bearing on the
                  representations learned by the CNN encoder. We have
                  proposed an end-to-end framework that clusters the
                  patches from a WSI into ${k}$-groups, samples ${k}'$
                  patches from each group for training, and uses an
                  adaptive attention mechanism for slide level
                  prediction; Cluster-to-Conquer (C2C). We have
                  demonstrated that dividing a WSI into clusters can
                  improve the model training by exposing it to diverse
                  discriminative features extracted from the
                  patches. We regularized the clustering mechanism by
                  introducing a KL-divergence loss between the
                  attention weights of patches in a cluster and the
                  uniform distribution. The framework is optimized
                  end-to-end on slide-level cross-entropy, patch-level
                  cross-entropy, and KL-divergence loss.},
  author =	 {Sharma, Yash and Shrivastava, Aman and Ehsan,
                  Lubaina and Moskaluk, Christopher A. and Syed, Sana
                  and Brown, Donald},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {682--698},
  title =	 {Cluster-to-Conquer: A Framework for End-to-End
                  Multi-Instance Learning for Whole Slide Image
                  Classification},
  openreview =	 {7i1-2oKIELU},
}

@inproceedings{shi21,
  abstract =	 {We systematically evaluate the performance of deep
                  learning models in the presence of diseases not
                  labeled for or present during training. First, we
                  evaluate whether deep learning models trained on a
                  subset of diseases (seen diseases) can detect the
                  presence of any one of a larger set of diseases. We
                  find that models tend to falsely classify diseases
                  outside of the subset (unseen diseases) as `no
                  disease''. Second, we evaluate whether models
                  trained on seen diseases can detect seen diseases
                  when co-occurring with diseases outside the subset
                  (unseen diseases). We find that models are still
                  able to detect seen diseases even when co-occurring
                  with unseen diseases. Third, we evaluate whether
                  feature representations learned by models may be
                  used to detect the presence of unseen diseases given
                  a small labeled set of unseen diseases. We find that
                  the penultimate layer provides useful features for
                  unseen disease detection. Our results can inform the
                  safe clinical deployment of deep learning models
                  trained on a non-exhaustive set of disease classes.},
  author =	 {Shi, Siyu and Malhi, Ishaan and Tran, Kevin and Ng,
                  Andrew Y. and Rajpurkar, Pranav},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {699--712},
  title =	 {Unseen Disease Detection for Deep Learning
                  Interpretation of Chest X-rays},
  openreview =	 {i-zxSlqneRu},
}

@inproceedings{simko21,
  abstract =	 {The contrast settings to select before acquiring
                  magnetic resonance imaging (MRI) signal depend
                  heavily on the subsequent tasks. As each contrast
                  highlights different tissues, automated segmentation
                  tools for example might be optimized for a certain
                  contrast. While for radiotherapy, multiple scans of
                  the same region with different contrasts can achieve
                  a better accuracy for delineating tumours and organs
                  at risk. Unfortunately, the optimal contrast for the
                  subsequent automated methods might not be known
                  during the time of signal acquisition, and
                  performing multiple scans with different contrasts
                  increases the total examination time and registering
                  the sequences introduces extra work and potential
                  errors. Building on the recent achievements of deep
                  learning in medical applications, the presented work
                  describes a novel approach for transferring any
                  contrast to any other. The novel model architecture
                  incorporates the signal equation for spin echo
                  sequences, and hence the model inherently learns the
                  unknown quantitative maps for proton density, $T1$
                  and $T2$ relaxation times ($PD$, $T1$ and $T2$,
                  respectively). This grants the model the ability to
                  retrospectively reconstruct spin echo sequences by
                  changing the contrast settings Echo and Repetition
                  Time ($TE$ and $TR$, respectively). The model learns
                  to identify the contrast of pelvic MR images,
                  therefore no paired data of the same anatomy from
                  different contrasts is required for training. This
                  means that the experiments are easily reproducible
                  with other contrasts or other patient
                  anatomies. Despite the contrast of the input image,
                  the model achieves accurate results for
                  reconstructing signal with contrasts available for
                  evaluation. For the same anatomy, the quantitative
                  maps are consistent for a range of contrasts of
                  input images. Realized in practice, the proposed
                  method would greatly simplify the modern
                  radiotherapy pipeline. The trained model is made
                  public together with a tool for testing the model on
                  example images.},
  author =	 {Simko, Attila Tibor and L{\"o}fstedt, Tommy and
                  Garpebring, Anders and Bylund, Mikael and Nyholm,
                  Tufve and Jonsson, Joakim},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {713--727},
  title =	 {Changing the Contrast of Magnetic Resonance Imaging
                  Signals using Deep Learning},
  openreview =	 {lWeQH4Kpsys},
}

@inproceedings{sowrirajan21,
  abstract =	 {Contrastive learning is a form of self-supervision
                  that can leverage unlabeled data to produce
                  pretrained models. While contrastive learning has
                  demonstrated promising results on natural image
                  classification tasks, its application to medical
                  imaging tasks like chest X-ray interpretation has
                  been limited. In this work, we propose MoCo-CXR,
                  which is an adaptation of the contrastive learning
                  method Momentum Contrast (MoCo), to produce models
                  with better representations and initializations for
                  the detection of pathologies in chest X-rays. In
                  detecting pleural effusion, we find that linear
                  models trained on MoCo-CXR-pretrained
                  representations outperform those without
                  MoCo-CXR-pretrained representations, indicating that
                  MoCo-CXR-pretrained representations are of
                  higher-quality. End-to-end fine-tuning experiments
                  reveal that a model initialized via
                  MoCo-CXR-pretraining outperforms its
                  non-MoCo-CXR-pretrained counterpart. We find that
                  MoCo-CXR-pretraining provides the most benefit with
                  limited labeled training data. Finally, we
                  demonstrate similar results on a target Tuberculosis
                  dataset unseen during pretraining, indicating that
                  MoCo-CXR-pretraining endows models with
                  representations and transferability that can be
                  applied across chest X-ray datasets and tasks.},
  author =	 {Sowrirajan, Hari and Yang, Jingbo and Ng, Andrew
                  Y. and Rajpurkar, Pranav},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {728--744},
  title =	 {MoCo Pretraining Improves Representation and
                  Transferability of Chest X-ray Models},
  openreview =	 {LO7Su0-dPJl},
}

@inproceedings{toelle21,
  abstract =	 {Exploiting the deep image prior property of
                  convolutional auto-encoder networks is especially
                  interesting for medical image processing as it
                  avoids hallucinations by omitting supervised
                  learning. Its spectral bias towards lower
                  frequencies makes it suitable for inverse image
                  problems such as denoising and super-resolution, but
                  manual early stopping has to be applied to act as a
                  low-pass filter. In this paper, we present a novel
                  Bayesian approach to deep image prior using
                  mean-field variational inference. This allows for
                  uncertainty quantification on a per-pixel level and,
                  given the right prior distribution on the network
                  weights, omits the need for early stopping. We
                  optimize the parameters of the weight prior towards
                  reconstruction accuracy using Bayesian optimization
                  with Gaussian Process regression. We evaluate our
                  approach on different inverse tasks on a variety of
                  modalities and demonstrate that an optimized weight
                  prior outperforms former state-of-the-art Bayesian
                  deep image prior approaches. We show that a badly
                  selected prior leads to worse accuracy and
                  calibration and that it is sufficient to optimize
                  the weight prior parameter per task domain.},
  author =	 {T{\"o}lle, Malte and Laves, Max-Heinrich and
                  Schlaefer, Alexander},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {745--760},
  title =	 {A Mean-Field Variational Inference Approach to Deep
                  Image Prior for Inverse Problems in Medical Imaging},
  openreview =	 {DvV_blKLiB4},
}


@inproceedings{turja21,
  abstract =	 {The excessive deposition of misfolded proteins such
                  as amyloid-$\beta$~(A$\beta$) protein is an aging
                  event underlying several neurodegenerative
                  diseases. Mounting evidence shows that the spreading
                  of neuropathological burden has a strong association
                  to the white matter tracts in the brain which can be
                  measured using diffusion-weighted imaging and
                  tractography technologies. Most of the previous
                  studies analyze the dynamic progression of amyloid
                  using cross-sectional data which is not robust to
                  the heterogeneous A$\beta$ dynamics across the
                  population. In this regard, we propose a graph
                  neural network-based learning framework to capture
                  the disease-related dynamics by tracking the
                  spreading of amyloid across brain networks from the
                  subject-specific longitudinal PET images. To learn
                  from limited (2 -- 3 timestamps) and noisy
                  longitudinal data, we restrict the space of amyloid
                  propagation patterns to a latent heat diffusion
                  model which is constrained by the anatomical
                  connectivity of the brain. Our experiments show that
                  restricting the dynamics to be a heat diffusion
                  mechanism helps to train a robust deep neural
                  network for predicting future time points and
                  classifying Alzheimer's disease brain.},
  author =	 {Turja, Md Asadullah and Wu, Guorong and Yang, Defu
                  and Styner, Martin Andreas},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {761--773},
  title =	 {Learning the Latent Heat Diffusion Process through
                  Structural Brain Network from Longitudinal
                  $\beta$-Amyloid Data},
  openreview =	 {S3QYCe74DPu},
}

@inproceedings{uzunova21,
  abstract =	 {Diffeomorphic and deforming autoencoders have been
                  recently explored in the field of medical imaging
                  for appearance and shape disentanglement. Both
                  models are based on the deformable template
                  paradigm, however they show different weaknesses for
                  the representation of medical images. Diffeomorphic
                  autoencoders only consider spatial deformations,
                  whereas deforming autoencoders also regard changes
                  in the appearance, however no uniform template is
                  generated for the whole training dataset, and the
                  appearance is modeled depending on a very few
                  parameters. In this work, we propose a method that
                  represents images based on a global template, where
                  next to the spatial displacement, the appearance is
                  modeled as the pixel-wise intensity difference to
                  the unified template. To however ensure that the
                  generated appearance offsets adhere to the template
                  shape, a guided filter smoothing of the appearance
                  map is integrated into an end-to-end training
                  process. This regularization significantly improves
                  the disentanglement of shape and appearance and thus
                  enables multi-modal image modeling. Furthermore, the
                  generated templates are crisper and the registration
                  accuracy improves. Our experiments also show
                  applications of the proposed approach in the field
                  of automatic population analysis.},
  author =	 {Uzunova, Hristina and Handels, Heinz and Ehrhardt,
                  Jan},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {774--786},
  title =	 {Guided Filter Regularization for Improved
                  Disentanglement of Shape and Appearance in
                  Diffeomorphic Autoencoders},
  openreview =	 {ILEMHPV_Lc2},
}

@inproceedings{vadacchino21,
  abstract =	 {Segmentation of enhancing tumours or lesions from
                  MRI is important for detecting new disease activity
                  in many clinical contexts. However, accurate
                  segmentation requires the inclusion of medical
                  images (e.g., T1 post-contrast MRI) acquired after
                  injecting patients with a contrast agent (e.g.,
                  Gadolinium), a process no longer thought to be
                  safe. Although a number of modality-agnostic
                  segmentation networks have been developed over the
                  past few years, they have been met with limited
                  success in the context of enhancing pathology
                  segmentation. In this work, we present HAD-Net, a
                  novel offline adversarial knowledge distillation
                  (KD) technique, whereby a pre-trained teacher
                  segmentation network, with access to all MRI
                  sequences, teaches a student network, via
                  hierarchical adversarial training, to better
                  overcome the large domain shift presented when
                  crucial images are absent during inference. In
                  particular, we apply HAD-Net to the challenging task
                  of enhancing tumour segmentation when access to
                  post-contrast imaging is not available. The proposed
                  network is trained and tested on the BraTS 2019
                  brain tumour segmentation challenge dataset, where
                  it achieves performance improvements in the ranges
                  of 16\% - 26\% over (a) recent modality-agnostic
                  segmentation methods (U-HeMIS, U-HVED), (b) KD-Net
                  adapted to this problem, (c) the pre-trained student
                  network and (d) a non-hierarchical version of the
                  network (AD-Net), in terms of Dice scores for
                  enhancing tumour (ET). The network also shows
                  improvements in tumour core (TC) Dice
                  scores. Finally, the network outperforms both the
                  baseline student network and AD-Net in terms of
                  uncertainty quantification for enhancing tumour
                  segmentation based on the BraTS 2019 uncertainty
                  challenge metrics. Our code is publicly available
                  at: https://github.com/SaverioVad/HAD_Net},
  author =	 {Vadacchino, Saverio and Mehta, Raghav and Sepahvand,
                  Nazanin Mohammadi and Nichyporuk, Brennan and Clark,
                  James J. and Arbel, Tal},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {787--801},
  title =	 {HAD-Net: A Hierarchical Adversarial Knowledge
                  Distillation Network for Improved Enhanced Tumour
                  Segmentation Without Post-Contrast Images},
  openreview =	 {48UgSFrNR2},
}

@inproceedings{vanharten21,
  abstract =	 {Motility of the small intestine is a valuable metric
                  in the evaluation of gastrointestinal
                  disorders. Cine-MRI of the abdomen is a non-invasive
                  imaging technique allowing evaluation of this
                  motility. While 2D cine-MR imaging is increasingly
                  used for this purpose in both clinical practice and
                  in research settings, the potential of 3D cine-MR
                  imaging has been largely underexplored. In the
                  absence of image analysis tools enabling
                  investigation of the intestines as 3D structures,
                  the assessment of motility in 3D cine-images is
                  generally limited to the evaluation of movement in
                  separate 2D slices. Hence, to obtain an untangled
                  representation of the small intestine in 3D
                  cine-MRI, we propose a method to extract a
                  centerline of the intestine, thereby allowing easier
                  (visual) assessment by human observers, as well as
                  providing a possible starting point for automatic
                  analysis methods quantifying peristaltic bowel
                  movement along intestinal segments. The proposed
                  method automatically tracks individual sections of
                  the small intestine in 3D space, using a stochastic
                  tracker built on top of a CNN-based orientation
                  classifier. We show that the proposed method
                  outperforms a non-stochastic iterative tracking
                  approach.},
  author =	 {van Harten, Louis and de Jonge, Catharina and
                  Stoker, Jaap and Isgum, Ivana},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {802--812},
  title =	 {Untangling the Small Intestine in 3D cine-{MRI}
                  using Deep Stochastic Tracking},
  openreview =	 {cfYAFR6s6iJ},
}

@inproceedings{wood21,
  abstract =	 {The growing demand for head magnetic resonance
                  imaging (MRI) examinations, along with a global
                  shortage of radiologists, has led to an increase in
                  the time taken to report head MRI scans around the
                  world.  For many neurological conditions, this delay
                  can result in increased morbidity and mortality.  An
                  automated triaging tool could reduce reporting times
                  for abnormal examinations by identifying
                  abnormalities at the time of imaging and
                  prioritizing the reporting of these scans.  In this
                  work, we present a convolutional neural network
                  (CNN) for detecting clinically-relevant
                  abnormalities in T2-weighted head MRI scans. Using a
                  validated neuroradiology report classifier, we
                  generated a labelled dataset of 43,754 scans from
                  two large UK hospitals for model training, and
                  demonstrate accurate classification (area under the
                  receiver operating curve (AUC) = 0.943) on a test
                  set of 800 scans labelled by a team of
                  neuroradiologists.  Importantly, when trained on
                  scans from only a single hospital the model
                  generalized to scans from the other hospital (delta
                  AUC <= 0.02).  A simulation study demonstrated that
                  our model would reduce the mean reporting time for
                  abnormal scans from 28 days to 14 days and from 9
                  days to 5 days at the two hospitals, demonstrating
                  feasibility for use in a clinical triage
                  environment.},
  author =	 {Wood, David A. and Kafiabadi, Sina and Al Busaidi,
                  Aisha and Guilhem, Emily and Montvila, Antanas and
                  Agarwal, Siddarth and Lynch, Jeremy and Townend,
                  Matthew and Barker, Gareth and Ourselin, Sebastian
                  and Cole, James H. and Booth, Thomas C.},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {813--841},
  title =	 {Automated triaging of head MRI examinations using
                  convolutional neural networks},
  openreview =	 {gh8qD_lAADe},
}

@inproceedings{zhang21a,
  abstract =	 {To develop deep learning-based models for automatic
                  analysis of histopathology whole slide images
                  (WSIs), the atomic entities to be directly processed
                  are often the smaller patches cropped from WSIs as
                  it is not always possible to feed a whole WSI to a
                  model given its enormous size. However, a trained
                  model tends to relate the slide-specific
                  characteristics to diagnosis results because a large
                  number of patches cropped from the same WSI will
                  share common slide features and thus have strong
                  correlations between them, resulting in deteriorated
                  generalization capability of the trained
                  model. Current approaches to alleviate this issue
                  include data pre-processing (stain normalization or
                  color augmentation) and adversarial learning, both
                  of which introduce extra complications in
                  computations. Alternatively, we propose to reduce
                  the impact of this issue by introducing a new
                  regularization term to the standard loss function to
                  reduce the correlation of the patches from the same
                  WSI. It is intuitive and easy-to-implement and
                  introduces comparably smaller computation overhead
                  compared to existing approaches. Experimental
                  results prove that the proposed regularization term
                  is able to enhance the generalization capability of
                  learning models and consequently to achieve better
                  performance. The code is available in:
                  \url{https://github.com/hrzhang1123/SlideCorrelationReduction}.},
  author =	 {Zhang, Hongrun and Meng, Yanda and Qian, Xuesheng
                  and Yang, Xiaoyun and Coupland, Sarah E. and Zheng,
                  Yalin},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {842--854},
  title =	 {A regularization term for slide correlation
                  reduction in whole slide image analysis with deep
                  learning},
  openreview =	 {2vCFIoWDS6E},
}

@inproceedings{zhang21b,
  abstract =	 {Confluent lesions usually occur when pathologically
                  distinct lesions grow close to each other and form a
                  large spatially-connected lesion. These confluent
                  lesions pose a great challenge for subsequent image
                  analysis and disease diagnosis, as individual
                  lesions are difficult to separate and segment. In
                  this paper, we propose a Memory U-Net that takes
                  advantage of recent fully convolutional neural
                  network U-Net and memory networks, to resolve the
                  issue. The main idea is that we develop a hybrid
                  model with a U-Net for feature extraction and a
                  memory network as the alternative code book for
                  generalized Hough voting. To alleviate the GPU
                  memory overhead brought by the large code book, we
                  decompose the large code book into three smaller
                  ones, where each one of them accounts for voting in
                  one specific direction.  Through voxel-wise voting,
                  a density map of lesion locations can be obtained by
                  aggregating votes from all lesion voxels, and this
                  density map is further used to generate final
                  instance segmentation results. Experiments on a
                  large-scale cross-sectional multiple sclerosis study
                  verify the efficiency and the effectiveness of the
                  proposed method.},
  author =	 {Zhang, Hang and Zhang, Jinwei and Yang, Gufeng and
                  Spincemaille, Pascal and Nguyen, Thanh D. and Wang,
                  Yi},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {855--867},
  title =	 {Memory U-Net: Memorizing Where to Vote for Lesion
                  Instance Segmentation},
  openreview =	 {JbWMYLN5Hba},
}

@inproceedings{zhang21c,
  abstract =	 {Surgical workflow recognition has been playing an
                  essential role in computer-assisted interventional
                  systems for modern operating rooms. In this paper,
                  we present a computer vision-based method named
                  SWNet that focuses on utilizing spatial information
                  and temporal information from the surgical video to
                  achieve surgical workflow recognition. As the first
                  step, we utilize Interaction-Preserved
                  Channel-Separated Convolutional Network (IP-CSN) to
                  extract features that contain spatial information
                  and local temporal information from the surgical
                  video through segments. Secondly, we train a
                  Multi-Stage Temporal Convolutional Network (MS-TCN)
                  with those extracted features to capture global
                  temporal information from the full surgical
                  video. Finally, by utilizing Prior Knowledge Noise
                  Filtering (PKNF), prediction noise from the output
                  of MS-TCN is filtered. We evaluate SWNet for Sleeve
                  Gastrectomy surgical workflow recognition. SWNet
                  achieves 90\% frame-level accuracy and reaches a
                  weighted Jaccard Score of 0.8256. This demonstrates
                  that SWNet has considerable potential to solve the
                  surgical workflow recognition problem.},
  author =	 {Zhang, Bokai and Ghanem, Amer and Simes, Alexander
                  and Choi, Henry and Yoo, Andrew and Min, Andrew},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {855--869},
  title =	 {{SWN}et: Surgical Workflow Recognition with Deep
                  Convolutional Network},
  openreview =	 {g1sESqlP214},
}

@inproceedings{zhang21d,
  abstract =	 {A Hybrid Optimization Between Iterative and network
                  fine-Tuning (HOBIT) reconstruction method is
                  proposed to solve quantitative susceptibility
                  mapping (QSM) inverse problem in MRI. In HOBIT, a
                  convolutional neural network (CNN) is first trained
                  on healthy subjects' data with gold standard
                  labels. Domain adaptation to patients' data with
                  hemorrhagic lesions is then deployed by minimizing
                  fidelity loss on the patient training
                  dataset. During test time, a fidelity loss is
                  imposed on each patient test case, where alternating
                  direction method of multiplier (ADMM) is used to
                  split the time consuming fidelity imposed network
                  update into iterative reconstruction and network
                  update subproblems alternatively in ADMM, and only a
                  subnet of the pre-trained CNN is updated during the
                  process. Compared to the method FINE where such
                  fidelity imposing strategy was initially proposed to
                  solve QSM, HOBIT achieved both performance gain of
                  reconstruction accuracy and vast reduction of
                  computational time.},
  author =	 {Zhang, Jinwei and Zhang, Hang and Spincemaille,
                  Pascal and Nguyen, Thanh and Sabuncu, Mert R. and
                  Wang, Yi},
  booktitle =	 {Medical Imaging with Deep Learning},
  pages =	 {870--880},
  title =	 {Hybrid optimization between iterative and network
                  fine-tuning reconstructions for fast quantitative
                  susceptibility mapping},
  openreview =	 {LFaxozc7Awm},
}
